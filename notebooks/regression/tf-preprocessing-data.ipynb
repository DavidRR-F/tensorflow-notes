{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data (Normalizatrion and Standardization)\n",
    "In terms of scaling values, neural networks tend to prefer normalization. If you are not sure on which to use, you could try both and see which performs better. Many Machine learning algorithms perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed such as:\n",
    "- linear and logistic regression\n",
    "- nearest neighbors\n",
    "- neural networks\n",
    "- support vector machines with radial bias kernel functions\n",
    "- principal components analysis\n",
    "- linear discriminent analysis\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "| Scaling Type | What it does | Scikit-Learn Function | When to use |\n",
    "|:------------:|:------------:|:---------------------:|:-----------:|\n",
    "| Scale (Normalization) | Converts all values to between 0 and 1 whilst preserving the original distribution | MinMaxScaler | Use as default scaler with neural networks |\n",
    "| Standardization | Removes the mean and divides each value by the standard deviation | StandardScaler | Transform feature to have close to normal distribution (caution: this reduces the effect of outliers) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "insurance = pd.read_csv('insurance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a cloumn transformer\n",
    "ct = make_column_transformer(\n",
    "    (MinMaxScaler(), ['age', 'bmi', 'children']), # normailize values between 0 and 1\n",
    "    (OneHotEncoder(), ['sex', 'smoker', 'region']) # one hot encode categorical variables\n",
    ")\n",
    "\n",
    "# Create X and y values\n",
    "X = insurance.drop('charges', axis=1)\n",
    "y = insurance['charges']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the column transformer to our training data\n",
    "ct.fit(X_train)\n",
    "\n",
    "# Transform training and test data with normalization and one hot encoding\n",
    "X_train_normal = ct.transform(X_train)\n",
    "X_test_normal = ct.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(age                19\n",
       " sex            female\n",
       " bmi              27.9\n",
       " children            0\n",
       " smoker            yes\n",
       " region      southwest\n",
       " Name: 0, dtype: object,\n",
       " array([0.60869565, 0.10734463, 0.4       , 1.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        ]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the transformed data \n",
    "X_train.loc[0] , X_train_normal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1070, 6), (1070, 11))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shapes\n",
    "X_train.shape, X_train_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21a0e7d1a80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a neural network on normalized data\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, input_shape=[11]),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.mae,\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_normal,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3442.244384765625, 3442.244384765625)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "# Since it was trained on normalized data it needs to be trained on normalized data\n",
    "loss, mae = model.evaluate(X_test_normal, y_test, verbose=0)\n",
    "loss, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
